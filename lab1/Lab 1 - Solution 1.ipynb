{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro_1",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Lab 1: Markov Decision Processes - Problem 1\n",
    "\n",
    "\n",
    "## Lab Instructions\n",
    "All your answers should be written in this notebook.  You shouldn't need to write or modify any other files.\n",
    "\n",
    "**You should execute every block of code to not miss any dependency.**\n",
    "\n",
    "\n",
    "*This project was developed by Peter Chen, Rocky Duan, Pieter Abbeel for the Berkeley Deep RL Bootcamp, August 2017. Bootcamp website with slides and lecture videos: https://sites.google.com/view/deep-rl-bootcamp/. It is adapted from Berkeley Deep RL Class [HW2](https://github.com/berkeleydeeprlcourse/homework/blob/c1027d83cd542e67ebed982d44666e0d22a00141/hw2/HW2.ipynb) [(license)](https://github.com/berkeleydeeprlcourse/homework/blob/master/LICENSE)*\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro_2",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This assignment will review the two classic methods for solving Markov Decision Processes (MDPs) with finite state and action spaces.\n",
    "We will implement value iteration (VI) and policy iteration (PI) for a finite MDP, both of which find the optimal policy in a finite number of iterations.\n",
    "\n",
    "The experiments here will use the Frozen Lake environment, a simple gridworld MDP that is taken from `gym` and slightly modified for this assignment. In this MDP, the agent must navigate from the start state to the goal state on a 4x4 grid, with stochastic transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc import FrozenLakeEnv, make_grader\n",
    "env = FrozenLakeEnv()\n",
    "print(env.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what a random episode looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Some basic imports and setup\n",
    "import numpy as np, numpy.random as nr, gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# Seed RNGs so you get the same printouts as me\n",
    "env.seed(0); from gym.spaces import prng; prng.seed(10)\n",
    "# Generate the episode\n",
    "env.reset()\n",
    "for t in range(100):\n",
    "    env.render()\n",
    "    a = env.action_space.sample()\n",
    "    ob, rew, done, _ = env.step(a)\n",
    "    if done:\n",
    "        break\n",
    "assert done\n",
    "env.render();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the episode above, the agent falls into a hole after two timesteps. Also note the stochasticity--on the first step, the DOWN action is selected, but the agent moves to the right.\n",
    "\n",
    "We extract the relevant information from the gym Env into the MDP class below.\n",
    "The `env` object won't be used any further, we'll just use the `mdp` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class MDP(object):\n",
    "    def __init__(self, P, nS, nA, desc=None):\n",
    "        self.P = P # state transition and reward probabilities, explained below\n",
    "        self.nS = nS # number of states\n",
    "        self.nA = nA # number of actions\n",
    "        self.desc = desc # 2D array specifying what each grid cell means (used for plotting)\n",
    "mdp = MDP( {s : {a : [tup[:3] for tup in tups] for (a, tups) in a2d.items()} for (s, a2d) in env.P.items()}, env.nS, env.nA, env.desc)\n",
    "\n",
    "\n",
    "print(\"mdp.P is a two-level dict where the first key is the state and the second key is the action.\")\n",
    "print(\"The 2D grid cells are associated with indices [0, 1, 2, ..., 15] from left to right and top to down, as in\")\n",
    "print(np.arange(16).reshape(4,4))\n",
    "print(\"Action indices [0, 1, 2, 3] correspond to West, South, East and North.\")\n",
    "print(\"mdp.P[state][action] is a list of tuples (probability, nextstate, reward).\\n\")\n",
    "print(\"For example, state 0 is the initial state, and the transition information for s=0, a=0 is \\nP[0][0] =\", mdp.P[0][0], \"\\n\")\n",
    "print(\"As another example, state 5 corresponds to a hole in the ice, in which all actions lead to the same state with probability 1 and reward 0.\")\n",
    "for i in range(4):\n",
    "    print(\"P[5][%i] =\" % i, mdp.P[5][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "4",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## Part 1: Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "### Problem 1: implement value iteration\n",
    "In this problem, you'll implement value iteration, which has the following pseudocode:\n",
    "\n",
    "---\n",
    "Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "For $i=0, 1, 2, \\dots$\n",
    "- $V^{(i+1)}(s) = \\max_a \\sum_{s'} P(s,a,s') [ R(s,a,s') + \\gamma V^{(i)}(s')]$, for all $s$\n",
    "\n",
    "---\n",
    "\n",
    "We additionally define the sequence of greedy policies $\\pi^{(0)}, \\pi^{(1)}, \\dots, \\pi^{(n-1)}$, where\n",
    "$$\\pi^{(i)}(s) = \\arg \\max_a \\sum_{s'} P(s,a,s') [ R(s,a,s') + \\gamma V^{(i)}(s')]$$\n",
    "\n",
    "Your code will return two lists: $[V^{(0)}, V^{(1)}, \\dots, V^{(n)}]$ and $[\\pi^{(0)}, \\pi^{(1)}, \\dots, \\pi^{(n-1)}]$\n",
    "\n",
    "To ensure that you get the same policies as the reference solution, choose the lower-index action to break ties in $\\arg \\max_a$. This is done automatically by np.argmax. This will only affect the \"# chg actions\" printout below--it won't affect the values computed.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Warning: make a copy of your value function each iteration and use that copy for the update--don't update your value function in place. \n",
    "Updating in-place is also a valid algorithm, sometimes called Gauss-Seidel value iteration or asynchronous value iteration, but it will cause you to get different results than our reference solution (which in turn will mean that our testing code won’t be able to help in verifying your code).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros(mdp.nA)\n",
    "y = np.ones(mdp.nA)\n",
    "(x != y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "vstar_backup",
     "locked": false,
     "solution": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def value_iteration(mdp, gamma, nIt, grade_print=print):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        mdp: MDP\n",
    "        gamma: discount factor\n",
    "        nIt: number of iterations, corresponding to n above\n",
    "    Outputs:\n",
    "        (value_functions, policies)\n",
    "        \n",
    "    len(value_functions) == nIt+1 and len(policies) == nIt\n",
    "    \"\"\"\n",
    "    grade_print(\"Iteration | max|V-Vprev| | # chg actions | V[0]\")\n",
    "    grade_print(\"----------+--------------+---------------+---------\")\n",
    "    Vs = [np.zeros(mdp.nS)] # list of value functions contains the initial value function V^{(0)}, which is zero\n",
    "    pis = []\n",
    "    for it in range(nIt):\n",
    "        oldpi = pis[-1] if len(pis) > 0 else None # \\pi^{(it)} = Greedy[V^{(it-1)}]. Just used for printout\n",
    "        Vprev = Vs[-1] # V^{(it)}\n",
    "        \n",
    "        # Your code should fill in meaningful values for the following two variables\n",
    "        # pi: greedy policy for Vprev (not V), \n",
    "        #     corresponding to the math above: \\pi^{(it)} = Greedy[V^{(it)}]\n",
    "        #     ** it needs to be numpy array of ints **\n",
    "        # V: bellman backup on Vprev\n",
    "        #     corresponding to the math above: V^{(it+1)} = T[V^{(it)}]\n",
    "        #     ** numpy array of floats **\n",
    "        \n",
    "        #######################\n",
    "        # BEGIN Solution Code #\n",
    "        #######################\n",
    "        \n",
    "        V = np.zeros(mdp.nS)\n",
    "        pi = np.zeros(mdp.nS)\n",
    "        \n",
    "        # For All States\n",
    "        for s in range(mdp.nS):\n",
    "            \n",
    "            A = np.zeros(mdp.nA)\n",
    "            \n",
    "            # For All Actions\n",
    "            for a in range(mdp.nA):\n",
    "                \n",
    "                # For Each State Transition and Reward Probability\n",
    "                for p, s_next, r in mdp.P[s][a]:\n",
    "                    A[a] += p*(r + gamma * Vprev[s_next])\n",
    "                    \n",
    "            V[s] = np.max(A)\n",
    "            pi[s] = np.argmax(A)\n",
    "        \n",
    "        #####################\n",
    "        # END Solution Code #\n",
    "        #####################\n",
    "        \n",
    "        max_diff = np.abs(V - Vprev).max()\n",
    "        nChgActions=\"N/A\" if oldpi is None else (pi != oldpi).sum()\n",
    "        grade_print(\"%4i      | %6.5f      | %4s          | %5.3f\"%(it, max_diff, nChgActions, V[0]))\n",
    "        Vs.append(V)\n",
    "        pis.append(pi)\n",
    "    return Vs, pis\n",
    "\n",
    "GAMMA = 0.95 # we'll be using this same value in subsequent problems\n",
    "\n",
    "\n",
    "# The following is the output of a correct implementation; when\n",
    "#   this code block is run, your implementation's print output will be\n",
    "#   compared with expected output.\n",
    "#   (incorrect line in red background with correct line printed side by side to help you debug)\n",
    "expected_output = \"\"\"Iteration | max|V-Vprev| | # chg actions | V[0]\n",
    "----------+--------------+---------------+---------\n",
    "   0      | 0.80000      |  N/A          | 0.000\n",
    "   1      | 0.60800      |    2          | 0.000\n",
    "   2      | 0.51984      |    2          | 0.000\n",
    "   3      | 0.39508      |    2          | 0.000\n",
    "   4      | 0.30026      |    2          | 0.000\n",
    "   5      | 0.25355      |    1          | 0.254\n",
    "   6      | 0.10478      |    0          | 0.345\n",
    "   7      | 0.09657      |    0          | 0.442\n",
    "   8      | 0.03656      |    0          | 0.478\n",
    "   9      | 0.02772      |    0          | 0.506\n",
    "  10      | 0.01111      |    0          | 0.517\n",
    "  11      | 0.00735      |    0          | 0.524\n",
    "  12      | 0.00310      |    0          | 0.527\n",
    "  13      | 0.00190      |    0          | 0.529\n",
    "  14      | 0.00083      |    0          | 0.530\n",
    "  15      | 0.00049      |    0          | 0.531\n",
    "  16      | 0.00022      |    0          | 0.531\n",
    "  17      | 0.00013      |    0          | 0.531\n",
    "  18      | 0.00006      |    0          | 0.531\n",
    "  19      | 0.00003      |    0          | 0.531\"\"\"\n",
    "Vs_VI, pis_VI = value_iteration(mdp, gamma=GAMMA, nIt=20, grade_print=make_grader(expected_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we've illustrated the progress of value iteration. Your optimal actions are shown by arrows.\n",
    "At the bottom, the value of the different states are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for (V, pi) in zip(Vs_VI[:10], pis_VI[:10]):\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(V.reshape(4,4), cmap='gray', interpolation='none', clim=(0,1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(4)-.5)\n",
    "    ax.set_yticks(np.arange(4)-.5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    Y, X = np.mgrid[0:4, 0:4]\n",
    "    a2uv = {0: (-1, 0), 1:(0, -1), 2:(1,0), 3:(-1, 0)}\n",
    "    Pi = pi.reshape(4,4)\n",
    "    for y in range(4):\n",
    "        for x in range(4):\n",
    "            a = Pi[y, x]\n",
    "            u, v = a2uv[a]\n",
    "            plt.arrow(x, y,u*.3, -v*.3, color='r', head_width=0.1, head_length=0.1) \n",
    "            plt.text(x, y, str(env.desc[y,x].item().decode()),\n",
    "                     color='g', size=12,  verticalalignment='center',\n",
    "                     horizontalalignment='center', fontweight='bold')\n",
    "    plt.grid(color='b', lw=2, ls='-')\n",
    "plt.figure()\n",
    "plt.plot(Vs_VI)\n",
    "plt.legend([str(i) for i in range(16)])\n",
    "plt.title(\"Values of different states\");"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
